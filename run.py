# -*- coding: utf-8 -*-
"""L90_zy317.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13d77tGmmY4c84nh_ssZr5bM_EGRfu7cs
"""

import tensorflow as tf
import tensorflow_addons as tfa

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time
import sys

import spacy
from spacy.tokenizer import Tokenizer

RANDOM_SEED = 1234

np.random.seed(RANDOM_SEED)

tf.random.set_seed(RANDOM_SEED)

nlp = spacy.load("en_core_web_sm")

import pandas as pd
wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'
train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])  # don't drop the empty lines yet, they show up as NaN in the data frame
train.head(n=30)

def add_pos(txt_orig):
  return txt
  txt = txt_orig.copy()
  for i in txt.index:
    token = txt.loc[i, 'token']
    pos = txt.loc[i, 'upos']
    if not pd.isnull(token):
      txt.at[i, 'token'] = token + '|' + pos
  return txt

def change_pos(txt_orig):
  txt = txt_orig.copy()
  return txt
  indices = []
  tokens = []
  pos_tags = []
  for i in txt.index:  # in each row...
    if not pd.isnull(txt.loc[i,'token']):
      indices.append(i)
      tokens.append(txt.loc[i,'token'])
    else:  # increment sequence counter at empty lines
      sent = ' '.join(tokens)
      nlp.tokenizer = Tokenizer(nlp.vocab)
      doc = nlp(sent)
      for w in doc:
        pos_tags.append(w.pos_)
      assert len(indices) == len(pos_tags)
      txt.at[indices, 'upos'] = pos_tags
      indices = []
      tokens = []
      pos_tags = []

  return txt

train_spacy = change_pos(train)
train_spacy.head(30)

from sklearn.metrics import cohen_kappa_score
cohen_kappa_score(train['upos'].dropna(), train_spacy['upos'].dropna())

# in order to convert word tokens to integers: list the set of token types
token_vocab = train_spacy.token.unique().tolist()
token_vocab.append('<sos>') # add start-of-sequence, max.index - 1
token_vocab.append('<eos>')# add end-of-sequence, max.index
oov = len(token_vocab)  # OOV (out of vocabulary) token as vocab length (because that's max.index + 1)

pos_vocab = train_spacy.upos.unique().tolist()
pos_vocab.append('<sos>') # add start-of-sequence, max.index - 1
pos_vocab.append('<eos>')# add end-of-sequence, max.index
pos_oov = len(pos_vocab)

# convert word tokens to integers
def token_index(tok):
  ind = tok
  if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet
    if tok in token_vocab:  # if token in vocabulary
      ind = token_vocab.index(tok)
    else:  # else it's OOV
      ind = oov
  return ind

# training labels: convert BIO to integers
def bio_index(bio):
  # 3 for <sos> and 4 for <eos>
  ind = bio
  if not pd.isnull(bio):  # deal with empty lines
    if bio=='B':
      ind = 0
    elif bio=='I':
      ind = 1
    elif bio=='O':
      ind = 2
  return ind

def pos_index(pos):
  ind = pos
  if not pd.isnull(pos):  # new since last time: deal with the empty lines which we didn't drop yet
    if pos in pos_vocab:  # if token in vocabulary
      ind = pos_vocab.index(pos)
    else:  # else it's OOV
      ind = pos_oov
  return ind

# pass a data frame through our feature extractor
def extract_features(txt_orig,istest=False):
  txt = txt_orig.copy()
  tokinds = [token_index(u) for u in txt['token']]
  txt['token_indices'] = tokinds
  posinds = [pos_index(u) for u in txt['upos']]
  txt['pos_indices'] = posinds
  if not istest:  # can't do this with the test set
    bioints = [bio_index(b) for b in txt['bio_only']]
    txt['bio_only'] = bioints
  return txt

train_copy = extract_features(train_spacy)
train_copy.head(n=30)

def tokens2sequences(txt_orig,istest=False):
  '''
  Takes panda dataframe as input, copies, and adds a sequence index based on full-stops.
  Outputs a dataframe with sequences of tokens, named entity labels, and token indices as lists.
  '''
  txt = txt_orig.copy()
  txt['sequence_num'] = 0
  seqcount = 0
  for i in txt.index:  # in each row...
    txt.loc[i,'sequence_num'] = seqcount  # set the sequence number
    if pd.isnull(txt.loc[i,'token']):  # increment sequence counter at empty lines
      seqcount += 1
  # now drop the empty lines, group by sequence number and output df of sequence lists
  txt = txt.dropna()
  if istest:  # test set doesn't have labels
    txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'token_indices', 'pos_indices']].agg(lambda x: list(x))
    for i in txt_seqs.index:
      seq = txt_seqs.loc[i, 'token']
      seq_ind = txt_seqs.loc[i, 'token_indices']
      pos_ind = txt_seqs.loc[i, 'pos_indices']
      txt_seqs.at[i, 'token'] = ['<sos>'] + seq + ['<eos>']
      txt_seqs.at[i, 'token_indices'] = [token_vocab.index('<sos>')] + seq_ind + [token_vocab.index('<eos>')]
      txt_seqs.at[i, 'pos_indices'] = [pos_vocab.index('<sos>')] + pos_ind + [pos_vocab.index('<eos>')]
  else:
    txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'bio_only', 'token_indices', 'pos_indices']].agg(lambda x: list(x))
    for i in txt_seqs.index:
      seq = txt_seqs.loc[i, 'token']
      bio = txt_seqs.loc[i, 'bio_only']
      seq_ind = txt_seqs.loc[i, 'token_indices']
      pos_ind = txt_seqs.loc[i, 'pos_indices']
      txt_seqs.at[i, 'token'] = ['<sos>'] + seq + ['<eos>']
      txt_seqs.at[i, 'bio_only'] = [3] + bio + [4]
      txt_seqs.at[i, 'token_indices'] = [token_vocab.index('<sos>')] + seq_ind + [token_vocab.index('<eos>')]
      txt_seqs.at[i, 'pos_indices'] = [pos_vocab.index('<sos>')] + pos_ind + [pos_vocab.index('<eos>')]
  return txt_seqs

print("This cell takes a little while to run: be patient :)")
train_seqs = tokens2sequences(train_copy)
train_seqs.head()

def find_longest_sequence(txt,longest_seq):
  '''find the longest sequence in the dataframe'''
  for i in txt.index:
    seqlen = len(txt['token'][i])
    if seqlen > longest_seq:  # update high water mark if new longest sequence encountered
      longest_seq = seqlen
  return longest_seq

train_longest = find_longest_sequence(train_seqs,0)
print('The longest sequence in the training set is %i tokens long' % train_longest)

# the dev set
wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'
dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])
dev_spacy = change_pos(dev)
dev_copy = extract_features(dev_spacy)
dev_seqs = tokens2sequences(dev_copy)
dev_longest = find_longest_sequence(dev_seqs,0)
print('The longest sequence in the dev set is %i tokens long' % dev_longest)

# the test set
wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'
test = pd.read_table(wnuttest, header=None, names=['token', 'upos'])
test_spacy = change_pos(test)
test_copy = extract_features(test_spacy, True)
test_seqs = tokens2sequences(test_copy, True)
test_longest = find_longest_sequence(test_seqs,0)
print('The longest sequence in the test set is %i tokens long' % test_longest)

from keras.preprocessing.sequence import pad_sequences

# set maximum sequence length
seq_length = test_longest

# a new dummy token index, one more than OOV
padtok = oov+1
print('The padding token index is %i' % padtok)

# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')
train_seqs_padded = pad_sequences(train_seqs['token_indices'].tolist(), maxlen=seq_length,
                                  dtype='int32', padding='post', truncating='post', value=padtok)
print('Example of padded token sequence:')
print(train_seqs_padded[1])

from keras.utils import to_categorical

# get lists of named entity labels, padded with a null label (=5)
padlab = 5
train_labs_padded = pad_sequences(train_seqs['bio_only'].tolist(), maxlen=seq_length,
                                  dtype='int32', padding='post', truncating='post', value=padlab)

# convert those labels to one-hot encoding
n_labs = 6  # we have 3 labels: B, I, O (0, 1, 2) + <sos> 3 + <eos> 4 + the pad label 5
train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]

# similarly for POS tag
padpos = pos_oov+1

train_pos_padded = pad_sequences(train_seqs['pos_indices'].tolist(), maxlen=seq_length,
                                  dtype='int32', padding='post', truncating='post', value=padpos)

# follow the print outputs below to see how the labels are transformed
print('Example of padded label sequence and one-hot encoding (first 10 tokens):')
print(train_seqs.loc[1])
print('Length of input sequence: %i' % len(train_labs_padded[1]))
print('Length of label sequence: %i' % len(train_labs_onehot[1]))
print('Length of pos sequence: %i' % len(train_pos_padded[1]))
print(train_labs_padded[1][:11])
print(train_labs_onehot[1][:11])
print(train_pos_padded[1][:11])

# now process the dev set in the same way: padding the tokens & labels, and one-hot encoding the labels
dev_seqs_padded = pad_sequences(dev_seqs['token_indices'].tolist(), maxlen=seq_length,
                                dtype='int32', padding='post', truncating='post', value=padtok)
dev_labs_padded = pad_sequences(dev_seqs['bio_only'].tolist(), maxlen=seq_length,
                                dtype='int32', padding='post', truncating='post', value=padlab)
dev_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in dev_labs_padded]
dev_pos_padded = pad_sequences(dev_seqs['pos_indices'].tolist(), maxlen=seq_length,
                                  dtype='int32', padding='post', truncating='post', value=padpos)

print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')
print(dev_seqs.loc[2])
print('Length of input sequence: %i' % len(dev_labs_padded[1]))
print('Length of label sequence: %i' % len(dev_labs_onehot[1]))
print('Length of label sequence: %i' % len(dev_pos_padded[1]))
print(dev_labs_padded[2][:11])
print(dev_labs_onehot[2][:11])
print(dev_pos_padded[2][:11])

from tensorflow import keras

# prepare sequences and labels as numpy arrays, check dimensions
X = np.array(train_seqs_padded)
X_pos = np.array(train_pos_padded)
y = np.array(train_labs_onehot)
print('Input sequence dimensions (n.docs, seq.length):')
print(X.shape)
print('Input pos tags dimensions (n.docs, seq.length):')
print(X_pos.shape)
print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')
print(y.shape)

# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)
vocab_size = padtok+1
print(vocab_size==len(token_vocab)+2)
pos_vocab_size = padpos+1
print(pos_vocab_size==len(pos_vocab)+2)
embed_size = 128  # try an embedding size of 128 (could tune this)

# final pos vocab size is the padding token + 1


# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve
METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
]

import os.path
if os.path.isfile('crf.py'):
  pass
else:
  !wget https://raw.githubusercontent.com/howl-anderson/addons/feature/crf_layer_on_stable_release/tensorflow_addons/layers/crf.py
  !sed -i '28d' 'crf.py'

from crf import CRF

def make_model(metrics = METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)

  word_input = keras.layers.Input(shape=(seq_length,))
  pos_input = keras.layers.Input(shape=(seq_length,))
  word_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True)(word_input)
  pos_emb = keras.layers.Embedding(input_dim=pos_vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True)(pos_input)
  merge_emb = keras.layers.Concatenate()([word_emb, pos_emb])
  bilstm = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(merge_emb)  # 2 directions, 50 units each, concatenated (can change this)
  dropout = keras.layers.Dropout(0.5)(bilstm)
  hidden1 = keras.layers.TimeDistributed(keras.layers.Dense(units=50, activation="relu"))(dropout)
  hidden2 = keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias))(dropout)
  # crf = CRF(n_labs, name='crf_layer')
  # output= crf(hidden1)

  model = keras.models.Model(inputs=[word_input, pos_input], outputs=[hidden2])
  # model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss={'crf_layer': crf.get_loss}, metrics=crf.get_accuracy)
  model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)
  return model

# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)

# the number of training epochs we'll use, and the batch size (how many texts are input at once)
EPOCHS = 100
BATCH_SIZE = 32

print('**Defining a neural network**')
model = make_model()
model.summary()

# figure out the label distribution in our fixed-length texts
from collections import Counter

all_labs = [l for lab in train_labs_padded for l in lab]
label_count = Counter(all_labs)
total_labs = len(all_labs)
print(label_count)
print(total_labs)

# use this to define an initial model bias
initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),
              (label_count[2]/total_labs), (label_count[3]/total_labs),
              (label_count[4]/total_labs), (label_count[5]/total_labs)]
print('Initial bias:')
print(initial_bias)

# pass the bias to the model and re-evaluate
model = make_model(output_bias=initial_bias)
# results = model.evaluate([X,X_pos], np.argmax(y,axis=-1), batch_size=BATCH_SIZE, verbose=0)
results = model.evaluate([X,X_pos], y, batch_size=BATCH_SIZE, verbose=0)
print("Loss: {:0.4f}".format(results[0]))

# use deep copy to ensure we aren't updating original values
import copy
train_weights_onehot = copy.deepcopy(train_labs_onehot)

# our first-pass class weights: normal for named entities (0 and 1), down-weighted for non named entities (2 and 3)
class_wts = [1,1,.1,.1,.1,.1]

# apply our weights to the label lists
for i,labs in enumerate(train_weights_onehot):
  for j,lablist in enumerate(labs):
    lablistaslist = lablist.tolist()
    whichismax = lablistaslist.index(max(lablistaslist))
    train_weights_onehot[i][j][whichismax] = class_wts[whichismax]

# what's this like, before and after?
print('Initial one-hot label encoding:')
print(train_labs_onehot[1][:11])

print('Weighted label encoding:')
print(train_weights_onehot[1][:11])

# prepare the dev sequences and labels as numpy arrays
dev_X = np.array(dev_seqs_padded)
dev_X_pos = np.array(dev_pos_padded)
dev_y = np.array(dev_labs_onehot)

# re-initiate model with bias
model = make_model(output_bias=initial_bias)

# now try the weighted one-hot encoding
y = np.array(train_weights_onehot)
print('Label dimensions (n.docs, seq.length, one-hot encoding of 6 NER labels):')
print(np.shape(y))

crf_early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_get_accuracy', verbose=1, patience=10, mode='max', restore_best_weights=True)

model = make_model(output_bias=initial_bias)
# model.fit([X, X_pos], np.argmax(y,axis=-1), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [crf_early_stopping], validation_data=([dev_X, dev_X_pos], np.argmax(dev_y,axis=-1)))
model.fit([X, X_pos], y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=([dev_X, dev_X_pos], dev_y))

# preds = np.argmax(model.predict([dev_seqs_padded, dev_pos_padded]), axis=-1)
# flat_preds = [p for pred in preds for p in pred]
# print(Counter(flat_preds))

preds = np.argmax(model.predict([dev_seqs_padded, dev_pos_padded]), axis=-1)
# preds = model.predict([dev_seqs_padded, dev_pos_padded])
flat_preds = [p for pred in preds for p in pred]
print(Counter(flat_preds))

# start a new column for the model predictions
dev_seqs['prediction'] = ''

# for each text: get original sequence length and trim predictions accordingly
# (_trim_ because we know that our seq length is longer than the longest seq in dev)
for i in dev_seqs.index:
  this_seq_length = len(dev_seqs['token'][i])
  dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)

dev_seqs.head()

# use sequence number as the index and apply pandas explode to all other columns
dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()
dev_long.head()

# re-using the BIO integer-to-character function from last time
def reverse_bio(ind):
  bio = 'O'  # for any sos=3, eos=4, pad=5 predictions
  if ind==0:
    bio = 'B'
  elif ind==1:
    bio = 'I'
  elif ind==2:
    bio = 'O'
  return bio

bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]
dev_long['bio_only'] = bio_labs
pred_labs = [reverse_bio(b) for b in dev_long['prediction']]
dev_long['prediction'] = pred_labs

dev_long.head()
dev_long.prediction.value_counts()

def wnut_evaluate(txt):
  '''row by row entity evaluation: we evaluate by whole named entities'''
  tp = 0; fp = 0; fn = 0
  in_entity = 0
  for i in txt.index:
    if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':
      if in_entity==1:  # if there's a preceding named entity which didn't have intervening O...
        tp += 1  # count a true positive
      in_entity = 1  # start tracking this entity (don't count it until we know full span of entity)
    elif txt['prediction'][i]=='B':
      fp += 1  # if not a B in gold annotations, it's a false positive
      in_entity = 0
    elif txt['prediction'][i]=='I' and txt['bio_only'][i]=='I':
      next  # correct entity continuation: do nothing
    elif txt['prediction'][i]=='I' and txt['bio_only'][i]=='B':
      fn += 1  # if a new entity should have begun, it's a false negative
      in_entity = 0
    elif txt['prediction'][i]=='I':  # if gold is O...
      if in_entity==1:  # and if tracking an entity, then the span is too long
        fp += 1  # it's a false positive
      in_entity = 0
    elif txt['prediction'][i]=='O':
      if txt['bio_only'][i]=='B':
        fn += 1  # false negative if there's B in gold but no predicted B
        if in_entity==1:  # also check if there was a named entity in progress
          tp += 1  # count a true positive
      elif txt['bio_only'][i]=='I':
        if in_entity==1:  # if this should have been a continued named entity, the span is too short
          fn += 1  # count a false negative
      elif txt['bio_only'][i]=='O':
        if in_entity==1:  # if a named entity has ended in right place
          tp += 1  # count a true positive
      in_entity = 0

  if in_entity==1:  # catch any final named entity
    tp += 1

  print('Sum of TP and FP = %i' % (tp+fp))
  print('Sum of TP and FN = %i' % (tp+fn))
  print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))
  prec = tp / (tp+fp)
  rec = tp / (tp+fn)
  f1 = (2*(prec*rec)) / (prec+rec)
  print('Precision = %.3f, Recall = %.3f, F1 = %.3f (max=1)' % (prec, rec, f1))

wnut_evaluate(dev_long)

# now process the test set in the same way: padding the tokens & labels, and one-hot encoding the labels
test_seqs_padded = pad_sequences(test_seqs['token_indices'].tolist(), maxlen=seq_length,
                                dtype='int32', padding='post', truncating='post', value=padtok)
test_pos_padded = pad_sequences(test_seqs['pos_indices'].tolist(), maxlen=seq_length,
                                  dtype='int32', padding='post', truncating='post', value=padpos)

test_preds = np.argmax(model.predict([test_seqs_padded, test_pos_padded]), axis=-1)
# preds = model.predict([dev_seqs_padded, dev_pos_padded])
test_flat_preds = [p for pred in test_preds for p in pred]
print(Counter(test_flat_preds))

test_seqs.tail()

# start a new column for the model predictions
test_seqs['prediction'] = ''

# for each text: get original sequence length and trim predictions accordingly
# (_trim_ because we know that our seq length is longer than the longest seq in dev)
for i in test_seqs.index:
  this_seq_length = len(test_seqs['token'][i])
  test_seqs['prediction'][i] = test_preds[i][:this_seq_length].astype(int)

test_seqs.head()

test_long = test_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()
test_long.head()

pred_labs = [reverse_bio(b) for b in test_long['prediction']]
test_long['prediction'] = pred_labs

test_long.head()
test_long.prediction.value_counts()

test_long.to_csv(r'test_result_zy317.txt', index=None, sep='\t')

wnut_evaluate(dev_long[dev_long['pos_indices']!=0])

dev_long_copy = dev_long.copy()
# for i in dev_long_copy.index:
#   if dev_long_copy.loc[i, 'pos_indices'] == 0:
#     dev_long_copy.at[i, 'prediction'] = 'I'

# for i in dev_long_copy.index:
#   if i < dev_long_copy.index[-1] and dev_long_copy.loc[i, 'prediction'] == 'O' and dev_long_copy.loc[i+1, 'prediction'] == 'I':
#     dev_long_copy.at[i+1, 'prediction'] = 'B'

for i in dev_long_copy.index:
  if i < dev_long_copy.index[-1] and dev_long_copy.loc[i, 'prediction'] == 'B' and dev_long_copy.loc[i+1, 'prediction'] == 'B':
    dev_long_copy.at[i+1, 'prediction'] = 'I'
  
wnut_evaluate(dev_long_copy)

dev1 = dev.copy()
dev1['prediction'] = 'O'
for i in dev1.index:
  if dev1.loc[i, 'upos'] == 'PROPN':
    dev1.at[i, 'prediction'] = 'I'

for i in dev1.index:
  if i < dev1.index[-1] and dev1.loc[i, 'prediction'] == 'O' and dev1.loc[i+1, 'prediction'] == 'I':
    dev1.at[i+1, 'prediction'] = 'B'
  
wnut_evaluate(dev1)

aa = dev_long[dev_long['bio_only'] != 'O']
dd = aa[aa['bio_only'] == aa['prediction']]

bb = aa[aa['token_indices'] == 14803]
cc = bb[bb['bio_only'] != bb['prediction']]

# ee = bb[bb['pos_indices'] != 9]
# ff = ee[ee['bio_only'] == ee['prediction']]

# eee = bb[bb['pos_indices'] == 9]
# fff = eee[eee['bio_only'] == eee['prediction']]

# bbb = aa[aa['pos_indices'] == 9]
# ccc = bbb[bbb['bio_only'] == bbb['prediction']]

# bbc = bbb[bbb['token_indices'] != 14803]
# ccb = bbc[bbc['bio_only'] == bbc['prediction']]

# print(len(aa))
# print(len(dd))
# print(len(bb))
# print(len(cc))
# print(len(ee))
# print(len(ff))
# print(len(eee))
# print(len(fff))

# print(len(bbb))
# print(len(ccc))

# print(len(bbc))
# print(len(ccb))

cc.head(50)

dev_long_copy = dev_long.copy()

for i in dev_long_copy.index:
  if i < dev_long_copy.index[-1] and dev_long_copy.loc[i, 'prediction'] == 'O' and dev_long_copy.loc[i+1, 'prediction'] == 'I':
    dev_long_copy.at[i+1, 'prediction'] = 'B'

wnut_evaluate(dev_long_copy)

def make_basic_model(metrics = METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)
  model = keras.Sequential([
      keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),
      keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)
      keras.layers.Dropout(0.5),
      keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),
  ])
  model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)
  return model

print('**Defining a neural network**')
basic_model = make_basic_model()
basic_model.summary()

basic_model = make_basic_model(output_bias=initial_bias)
basic_model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))

preds = np.argmax(basic_model.predict(dev_seqs_padded), axis=-1)
flat_preds = [p for pred in preds for p in pred]
print(Counter(flat_preds))

# start a new column for the model predictions
dev_seqs['prediction'] = ''

# for each text: get original sequence length and trim predictions accordingly
# (_trim_ because we know that our seq length is longer than the longest seq in dev)
for i in dev_seqs.index:
  this_seq_length = len(dev_seqs['token'][i])
  dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)

dev_seqs.head()

# use sequence number as the index and apply pandas explode to all other columns
dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()
dev_long.head(50)

bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]
dev_long['bio_only'] = bio_labs
pred_labs = [reverse_bio(b) for b in dev_long['prediction']]
dev_long['prediction'] = pred_labs

dev_long.head()
dev_long.prediction.value_counts()

wnut_evaluate(dev_long)